{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\distributions\\distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\distributions\\bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow import random\n",
    "random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data-SMS.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Class   400 non-null    object\n",
      " 1   Text    400 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 6.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'Class': 'label', 'Text': 'text'})\n",
    "data['label'] = data['label'].map({'real': 0, 'defraud': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   400 non-null    int64 \n",
      " 1   text    400 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 6.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>รับเลขมงคล และผลสลากทุกงวด 5บ/ข้อความ สมัคร *3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ดวงเป๊ะปัง รับทุกวัน เฮง ๆ รวย ๆ 5บ/ข้อความ สม...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ฟรีเด้อ! เว้าบาป-ตรี ชัยณรงค์ แถมฟรีค่าบริการเ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ฟรี! เพลงรอสาย บุคคลทั่วไป-ลำเพลิน วงศกร แถมฟร...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10.10 โหลดฟรี! เพลงรอสาย เวลา-ป๊อบ ปองกูล แถมฟ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>แพ็กที่ซื้อ 23/10/22 ใกล้จะหมดอายุแล้ว, รับฟรี...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>อย่าลืม! คุณมีเงินคืน 16.00บ.จะหมดอายุเวลา 09:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0</td>\n",
       "      <td>คุณได้เติมเงินจำนวน 50บ.ด้วยบัตรเงินสดทรูมันนี...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>อย่าลืมจ่ายบิลทรูของคุณ พิเศษเหมือนเคย รับเงิน...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;#&gt; OTP 111604 (Ref:ARPK) ห้ามแจ้งรหัสกับบุคคล...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0        1  รับเลขมงคล และผลสลากทุกงวด 5บ/ข้อความ สมัคร *3...\n",
       "1        1  ดวงเป๊ะปัง รับทุกวัน เฮง ๆ รวย ๆ 5บ/ข้อความ สม...\n",
       "2        1  ฟรีเด้อ! เว้าบาป-ตรี ชัยณรงค์ แถมฟรีค่าบริการเ...\n",
       "3        1  ฟรี! เพลงรอสาย บุคคลทั่วไป-ลำเพลิน วงศกร แถมฟร...\n",
       "4        1  10.10 โหลดฟรี! เพลงรอสาย เวลา-ป๊อบ ปองกูล แถมฟ...\n",
       "..     ...                                                ...\n",
       "395      0  แพ็กที่ซื้อ 23/10/22 ใกล้จะหมดอายุแล้ว, รับฟรี...\n",
       "396      0  อย่าลืม! คุณมีเงินคืน 16.00บ.จะหมดอายุเวลา 09:...\n",
       "397      0  คุณได้เติมเงินจำนวน 50บ.ด้วยบัตรเงินสดทรูมันนี...\n",
       "398      0  อย่าลืมจ่ายบิลทรูของคุณ พิเศษเหมือนเคย รับเงิน...\n",
       "399      0  <#> OTP 111604 (Ref:ARPK) ห้ามแจ้งรหัสกับบุคคล...\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pythainlp\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "'''''\n",
    "def tokenize(text):\n",
    "    thai_words = pythainlp.word_tokenize(text, engine='newmm')\n",
    "    eng_words = word_tokenize(text)\n",
    "    words = thai_words + eng_words\n",
    "    return words\n",
    "\n",
    "def tokenize(text):\n",
    "    thai_words = pythainlp.word_tokenize(text, engine='newmm')\n",
    "    eng_words = word_tokenize(text)\n",
    "    words = [word.strip() for word in thai_words + eng_words]\n",
    "    return words\n",
    "'''''\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(r'\\d+', 'num', text)\n",
    "    thai_words = pythainlp.word_tokenize(text,keep_whitespace=False, engine='newmm')\n",
    "    eng_words = word_tokenize(text)\n",
    "    #words = [word.replace(' ','' ) for word in thai_words + eng_words]\n",
    "    words = thai_words + eng_words\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'\\'\\nimport string\\nimport pythainlp\\nfrom pythainlp.tokenize import word_tokenize\\n\\ndef tokenize(text, keep_whitespace=False, keep_punctuations=False):\\n    thai_words = pythainlp.word_tokenize(text, engine=\\'newmm\\', keep_whitespace=keep_whitespace)\\n    eng_words = word_tokenize(text, keep_whitespace=keep_whitespace)\\n    \\n    # ตรวจสอบว่าต้องการเก็บตัวอักษรพิเศษหรือไม่\\n    if not keep_punctuations:\\n        thai_words = [word for word in thai_words if not is_punct(word)]\\n        eng_words = [word for word in eng_words if not is_punct(word)]\\n    \\n    words = thai_words + eng_words\\n    return words\\n\\ndef is_punct(word):\\n    \"\"\"\\n    ตรวจสอบว่าเป็นตัวอักษรพิเศษหรือไม่\\n    \"\"\"\\n    return all(char in string.punctuation for char in word)\\n\\n'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''''\n",
    "import string\n",
    "import pythainlp\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text, keep_whitespace=False, keep_punctuations=False):\n",
    "    thai_words = pythainlp.word_tokenize(text, engine='newmm', keep_whitespace=keep_whitespace)\n",
    "    eng_words = word_tokenize(text, keep_whitespace=keep_whitespace)\n",
    "    \n",
    "    # ตรวจสอบว่าต้องการเก็บตัวอักษรพิเศษหรือไม่\n",
    "    if not keep_punctuations:\n",
    "        thai_words = [word for word in thai_words if not is_punct(word)]\n",
    "        eng_words = [word for word in eng_words if not is_punct(word)]\n",
    "    \n",
    "    words = thai_words + eng_words\n",
    "    return words\n",
    "\n",
    "def is_punct(word):\n",
    "    \"\"\"\n",
    "    ตรวจสอบว่าเป็นตัวอักษรพิเศษหรือไม่\n",
    "    \"\"\"\n",
    "    return all(char in string.punctuation for char in word)\n",
    "\n",
    "'''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12.12',\n",
       " ' ',\n",
       " 'ซื้อ',\n",
       " ' ',\n",
       " 'E-Voucher',\n",
       " ' ',\n",
       " '500',\n",
       " 'บ.',\n",
       " 'ได้',\n",
       " '600',\n",
       " 'บ.',\n",
       " 'ซื้อ',\n",
       " 'เลย',\n",
       " '!>',\n",
       " 'https',\n",
       " '://',\n",
       " 'w',\n",
       " '.',\n",
       " 'ems',\n",
       " '.',\n",
       " 'to',\n",
       " '/',\n",
       " 'ef',\n",
       " '0',\n",
       " 'mjBX']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythainlp.word_tokenize(\"12.12 ซื้อ E-Voucher 500บ.ได้600บ.ซื้อเลย!>https://w.ems.to/ef0mjBX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12.12',\n",
       " 'ซื้อ',\n",
       " 'E-Voucher',\n",
       " '500บ.ได้600บ.ซื้อเลย',\n",
       " '!',\n",
       " '>',\n",
       " 'https',\n",
       " ':',\n",
       " '//w.ems.to/ef0mjBX']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"12.12 ซื้อ E-Voucher 500บ.ได้600บ.ซื้อเลย!>https://w.ems.to/ef0mjBX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num',\n",
       " '.',\n",
       " 'num',\n",
       " 'ซื้อ',\n",
       " 'E-Vouchernum',\n",
       " 'num',\n",
       " 'บ.',\n",
       " 'ได้',\n",
       " 'num',\n",
       " 'บ.',\n",
       " 'ซื้อ',\n",
       " 'เลย',\n",
       " '!>',\n",
       " 'https',\n",
       " '://',\n",
       " 'w',\n",
       " '.',\n",
       " 'ems',\n",
       " '.',\n",
       " 'to',\n",
       " '/',\n",
       " 'efnummjBX',\n",
       " 'num.num',\n",
       " 'ซื้อ',\n",
       " 'E-Vouchernum',\n",
       " 'numบ.ได้numบ.ซื้อเลย',\n",
       " '!',\n",
       " '>',\n",
       " 'https',\n",
       " ':',\n",
       " '//w.ems.to/efnummjBX']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"12.12 ซื้อ E-Voucher400 500บ.ได้600บ.ซื้อเลย!>https://w.ems.to/ef0mjBX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=tokenize)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 2191)\n",
      "(80, 2191)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vectorized.shape)\n",
    "print(X_test_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16996657, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.13208716, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vectorized.toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes classifier: 0.975\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# split data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], random_state=0)\n",
    "\n",
    "# vectorize data\n",
    "#vectorizer = CountVectorizer()\n",
    "#X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "#X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vectorized, y_train)\n",
    "y_pred_nb = nb.predict(X_test_vectorized)\n",
    "\n",
    "# evaluate model\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print('Accuracy of Naive Bayes classifier:', accuracy_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 2191)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vectorized._shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        36\n",
      "           1       1.00      0.95      0.98        44\n",
      "\n",
      "    accuracy                           0.97        80\n",
      "   macro avg       0.97      0.98      0.97        80\n",
      "weighted avg       0.98      0.97      0.98        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred_nb)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix Naive Bayes : \n",
      " [[36  0]\n",
      " [ 2 42]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "acc_score = accuracy_score(y_test, y_pred_nb)\n",
    "conf_mat_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "print(\"confusion_matrix Naive Bayes : \\n\",conf_mat_nb)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest classifier: 0.9375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# vectorize data\n",
    "#vectorizer = CountVectorizer()\n",
    "#X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "#X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_vectorized, y_train)\n",
    "y_pred_rf = rf.predict(X_test_vectorized)\n",
    "\n",
    "# evaluate model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print('Accuracy of Random Forest classifier:', accuracy_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93        36\n",
      "           1       0.95      0.93      0.94        44\n",
      "\n",
      "    accuracy                           0.94        80\n",
      "   macro avg       0.94      0.94      0.94        80\n",
      "weighted avg       0.94      0.94      0.94        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "print(report_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix Naive Bayes : \n",
      " [[34  2]\n",
      " [ 3 41]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "acc_score = accuracy_score(y_test, y_pred_rf)\n",
    "conf_mat_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "print(\"confusion_matrix Naive Bayes : \\n\",conf_mat_rf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pythainlp\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    thai_words = pythainlp.word_tokenize(text, engine='newmm')\n",
    "    eng_words = word_tokenize(text)\n",
    "    words = thai_words + eng_words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier: 0.9625\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# vectorize data\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# SVM\n",
    "model_SVM = SVC(kernel='linear')\n",
    "model_SVM.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# evaluate model\n",
    "y_pred_SVM = model_SVM.predict(X_test_vectorized)\n",
    "accuracy_SVM = accuracy_score(y_test, y_pred_SVM)\n",
    "print('Accuracy of SVM classifier:', accuracy_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96        36\n",
      "           1       0.98      0.95      0.97        44\n",
      "\n",
      "    accuracy                           0.96        80\n",
      "   macro avg       0.96      0.96      0.96        80\n",
      "weighted avg       0.96      0.96      0.96        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report_SVM = classification_report(y_test, y_pred_SVM)\n",
    "print(report_SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix Naive Bayes : \n",
      " [[35  1]\n",
      " [ 2 42]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "acc_score = accuracy_score(y_test, y_pred_SVM)\n",
    "conf_mat_SVM = confusion_matrix(y_test, y_pred_SVM)\n",
    "\n",
    "print(\"confusion_matrix Naive Bayes : \\n\",conf_mat_SVM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pythainlp\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# อ่านข้อมูล SMS จากไฟล์ CSV\n",
    "#data = pd.read_csv('Data-SMS.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.rename(columns={'Class': 'label', 'Text': 'text'})\n",
    "#data['label'] = data['label'].map({'real': 0, 'defraud': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r'\\d+', 'num', text)\n",
    "    thai_words = pythainlp.word_tokenize(text,keep_whitespace=False, engine='newmm')\n",
    "    eng_words = word_tokenize(text)\n",
    "    #words = [word.replace(' ','' ) for word in thai_words + eng_words]\n",
    "    words = thai_words + eng_words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แปลงข้อมูลเป็นตัวเลขโดยใช้ Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].apply(lambda x: ' '.join(pythainlp.word_tokenize(x, engine='newmm',keep_whitespace=False)) + ' '.join(word_tokenize(x))).values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].apply(lambda x: ' '.join(pythainlp.word_tokenize(x, engine='newmm',keep_whitespace=False)) + ' '.join(word_tokenize(x))).values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แปลงป้ายกำกับเป็นตัวเลข 0 หรือ 1 (ham หรือ spam)\n",
    "y = pd.get_dummies(data['label']).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แบ่งชุดข้อมูลเป็นชุดสำหรับ Train และ Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 118, 128)          640000    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 118, 128)          0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 771,842\n",
      "Trainable params: 771,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# กำหนดโมเดล LSTM\n",
    "\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(Embedding(5000, 128, input_length=X.shape[1]))\n",
    "model_LSTM.add(Dropout(0.3))\n",
    "model_LSTM.add(LSTM(128))\n",
    "model_LSTM.add(Dropout(0.3))\n",
    "model_LSTM.add(Dense(2, activation='softmax'))\n",
    "#model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_LSTM.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 4s 435ms/step - loss: 0.6888 - accuracy: 0.6938 - val_loss: 0.6770 - val_accuracy: 0.8875\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 270ms/step - loss: 0.6585 - accuracy: 0.8406 - val_loss: 0.6306 - val_accuracy: 0.7000\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 260ms/step - loss: 0.5683 - accuracy: 0.8594 - val_loss: 0.4797 - val_accuracy: 0.9250\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 266ms/step - loss: 0.3978 - accuracy: 0.9156 - val_loss: 0.3591 - val_accuracy: 0.9500\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 264ms/step - loss: 0.3190 - accuracy: 0.9812 - val_loss: 0.3192 - val_accuracy: 0.9500\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 264ms/step - loss: 0.2390 - accuracy: 0.9844 - val_loss: 0.2328 - val_accuracy: 0.9500\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 273ms/step - loss: 0.1548 - accuracy: 0.9750 - val_loss: 0.1828 - val_accuracy: 0.9500\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 278ms/step - loss: 0.1095 - accuracy: 0.9937 - val_loss: 0.1995 - val_accuracy: 0.9500\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 1s 259ms/step - loss: 0.0995 - accuracy: 0.9937 - val_loss: 0.1859 - val_accuracy: 0.9500\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 1s 265ms/step - loss: 0.0691 - accuracy: 0.9969 - val_loss: 0.1612 - val_accuracy: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f7001dc5e0>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ฝึกโมเดล\n",
    "model_LSTM.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001F70006FAC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 1s 24ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        36\n",
      "           1       0.95      0.95      0.95        44\n",
      "\n",
      "    accuracy                           0.95        80\n",
      "   macro avg       0.95      0.95      0.95        80\n",
      "weighted avg       0.95      0.95      0.95        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_LSTM.predict(X_test)\n",
    "\n",
    "# Convert the predictions to binary values\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Generate the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_LSTM = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_LSTM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix Naive Bayes : \n",
      " [[34  2]\n",
      " [ 2 42]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "conf_mat_LSTM = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"confusion_matrix Naive Bayes : \\n\",conf_mat_LSTM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "รวม"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes  : 0.975\n",
      "Accuracy of RandomForest : 0.9375\n",
      "Accuracy of SVM          : 0.9625\n",
      "Accuracy of LSTM         : 0.95\n"
     ]
    }
   ],
   "source": [
    "#การแยกข้อมูลด้วย TfidfVectorizer\n",
    "\n",
    "print('Accuracy of Naive Bayes  :', accuracy_nb)\n",
    "print('Accuracy of RandomForest :', accuracy_rf)\n",
    "print('Accuracy of SVM          :', accuracy_SVM)\n",
    "print('Accuracy of LSTM         :', accuracy_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix Naive Bayes : \n",
      " [[36  0]\n",
      " [ 2 42]]\n",
      "confusion_matrix Random Forest : \n",
      " [[34  2]\n",
      " [ 3 41]]\n",
      "confusion_matrix SVM  : \n",
      " [[35  1]\n",
      " [ 2 42]]\n",
      "confusion_matrix LSTM : \n",
      " [[34  2]\n",
      " [ 2 42]]\n"
     ]
    }
   ],
   "source": [
    "print(\"confusion_matrix Naive Bayes : \\n\",conf_mat_nb)\n",
    "\n",
    "print(\"confusion_matrix Random Forest : \\n\",conf_mat_rf)\n",
    "\n",
    "print(\"confusion_matrix SVM  : \\n\",conf_mat_SVM)\n",
    "\n",
    "print(\"confusion_matrix LSTM : \\n\",conf_mat_LSTM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import pythainlp\n",
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = pd.read_csv('data_test.csv')\n",
    "print(sms.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r'\\d+', 'num', text)\n",
    "    thai_words = pythainlp.word_tokenize(text,keep_whitespace=False, engine='newmm')\n",
    "    eng_words = word_tokenize(text)\n",
    "    #words = [word.replace(' ','' ) for word in thai_words + eng_words]\n",
    "    words = thai_words + eng_words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['words'] = sms['sms_text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join([word for words in sms['words'] for word in words])\n",
    "wordcloud = WordCloud(width=800, height=800,\n",
    "                      background_color='white',\n",
    "                      font_path='THSarabunNew.ttf',\n",
    "                      stopwords=STOPWORDS,\n",
    "                      min_font_size=10,\n",
    "                      colormap='Dark2').generate(text)\n",
    "\n",
    "# แสดง Word Cloud\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "  \n",
    "plt.show()\n",
    "plt.savefig(\"1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# รวมเป็นข้อความเดียว\n",
    "all_words = ' '.join([word for words in sms['words'] for word in words])\n",
    "\n",
    "# สร้าง WordCloud object\n",
    "wordcloud = WordCloud(font_path='THSarabunNew.ttf',str='black', background_color='white', width=800, height=400, colormap='tab20b').generate(all_words)\n",
    "\n",
    "# แสดง WordCloud\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.savefig(\"1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# อ่านข้อมูล SMS จากไฟล์ text\n",
    "with open(\"sms.txt\", encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# แยกคำที่เป็นภาษาไทยและอังกฤษเท่านั้น\n",
    "thai_words = [word for word in word_tokenize(text, engine='newmm') if word.isalpha() and word != \" \"]\n",
    "eng_words = [word for word in word_tokenize(text) if word.isalpha() and word != \" \"]\n",
    "\n",
    "# รวมคำภาษาไทยและอังกฤษเข้าด้วยกัน\n",
    "words = thai_words + eng_words\n",
    "\n",
    "# นับความถี่ของคำแต่ละคำ\n",
    "word_counts = dict()\n",
    "for word in words:\n",
    "    if word not in word_counts:\n",
    "        word_counts[word] = 1\n",
    "    else:\n",
    "        word_counts[word] += 1\n",
    "\n",
    "# สร้าง Word Cloud\n",
    "wordcloud = WordCloud(background_color=\"white\", width=800, height=400, colormap=\"Blues\", max_words=100).generate_from_frequencies(word_counts)\n",
    "\n",
    "# แสดง Word Cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.savefig('2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = pd.read_csv('data_test.csv')\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    thai_words = pythainlp.word_tokenize(text,keep_whitespace=False, engine='newmm')\n",
    "    return words\n",
    "\n",
    "sms['words'] = sms['sms_text'].apply(tokenize)\n",
    "\n",
    "text = ' '.join([word for words in sms['words'] for word in words])\n",
    "wordcloud = WordCloud(width=800, height=800,\n",
    "                      background_color='white',\n",
    "                      font_path='THSarabunNew.ttf',\n",
    "                      stopwords=STOPWORDS,\n",
    "                      min_font_size=10,\n",
    "                      colormap='Dark2').generate(text)\n",
    "\n",
    "# แสดง Word Cloud\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "  \n",
    "plt.show()\n",
    "plt.savefig(\"1.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55425978113d0db74a03eaf515d7265fb352aa9ac7a98f4c65dfefca375bf712"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
